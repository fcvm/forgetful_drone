\chapter{Autonomous Navigation Method}
\label{mainone}
\textit{
DELETEME: In this chapter you start addressing your actual problem. Therefore, it makes often sense to make a detailed problem analysis first (if not done in introduction). You should be sure about what to do and how. As writtin in the background part, it might also make sense to include complex background information or papers you are basing on in this analysis. If you are solving a software problem, you should follow the state of the art of software development which basically includes: problem analysis, design, implementation, testing, and deployment. Maintenance is often also described but I believe this will not be required for most theses. Code should be placed in the appendix unless it is solving an essential aspect of your work.
}





In order to investigate the effect of temporal comprehension in machine-learning-based, autonomous navigation of drones,
this master thesis extends the navigation method of Kaufmann, Loquercio et. al [?] is minimally adjusted and extended with a recurrent convolutional neural network.
This chapter \dots



\section{Reference systems}
In this thesis, the coordinates of a point 
$\pos[]{}{}{}{}$
relate to either the global, the local or the image reference system
\begin{equation}
    \pos[]{}{}{\grs}{} = \begin{bmatrix}
        \x[]{}{}{\grs}{} \\ \y[]{}{}{\grs}{} \\ \z[]{}{}{\grs}{}
    \end{bmatrix} \in \mathbb{R}^3
    ,\quad 
    \pos[]{}{}{\lrs}{} = \begin{bmatrix}
        \x[]{}{}{\lrs}{} \\ \y[]{}{}{\lrs}{} \\ \z[]{}{}{\lrs}{}
    \end{bmatrix} \in \mathbb{R}^3
    ,\quad 
    \pos[]{}{}{\irs}{} = \begin{bmatrix}
        \x[]{}{}{\irs}{} \\ \y[]{}{}{\irs}{}
    \end{bmatrix} \in \left[\text -1, 1\right]^2.
\end{equation}


The 3D global reference system
is fixed to an arbitrary point on earth
and is hence quasi inertial.
It is spanned by the orthonormal basis
which, related to the global reference system,
equates to the standard basis of $\mathbb{R}^3$
\begin{equation}
    \left\{
        \unitvec[]{\text G}{x}{}{},
        \unitvec[]{\text G}{y}{}{},
        \unitvec[]{\text G}{z}{}{}
    \right\}
    \quad \text{with} \quad 
    \unitvec[]{\text G}{x}{\grs}{} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix},\ 
    \unitvec[]{\text G}{y}{\grs}{} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix},\ 
    \unitvec[]{\text G}{z}{\grs}{} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}.
\end{equation}


The local reference system (see figure \ref{fig:local_reference_system}) is fixed to the moving drone.
It is spanned by the orthonormal basis,
whose origin is located at the optical center of the drone's onboard camera,
\begin{equation}
    \left\{
        \unitvec[]{\text L}{x}{}{},
        \unitvec[]{\text L}{y}{}{},
        \unitvec[]{\text L}{z}{}{}
    \right\}
    \quad \text{with} \quad 
    \unitvec[]{\text L}{x}{\lrs}{} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix},\ 
    \unitvec[]{\text L}{y}{\lrs}{} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix},\ 
    \unitvec[]{\text L}{z}{\lrs}{} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}.
\end{equation}
The unit vector 
$\unitvec[]{\text L}{x}{}{}$ 
points along the optical axis of the camera
in the flight direction of the drone.
The unit vector
$\unitvec[]{\text L}{z}{}{}$ 
points in the direction of the forces generated by the drone's rotors
and is parallel to the vertical axis of the image plane of the drone's onboard camera.
The unit vector 
$\unitvec[]{\text L}{y}{}{}$ 
points to the left of the drone
and parallels the horizontal axis of the image plane.






The image reference system (see figure \ref{fig:image_reference_system}) 
is superimposed on the images of the drone's onboard camera.
This 2-dimensional system is spanned by the orthonormal basis
\begin{equation}
    \left\{
        \unitvec[]{\text I}{x}{}{},
        \unitvec[]{\text I}{y}{}{}
    \right\}
    \quad \text{with} \quad 
    \unitvec[]{\text I}{x}{\irs}{} = \begin{bmatrix} 1 \\ 0 \end{bmatrix},\ 
    \unitvec[]{\text I}{y}{\irs}{} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
\end{equation}
The origin of the image reference system 
is located at the center of the image plane.
The unit vector 
$\unitvec[]{\text I}{x}{}{}$  
points rightwards along the vertical axis of the image plane.
The unit vector 
$\unitvec[]{\text I}{y}{}{}$  
points upwards along the horizontal axis of the image plane.
A point on the image plane 
is bounded by the left and right
$ -1 \le \x[]{}{}{\irs}{} \le 1 $
as well as the lower and upper
$ -1 \le \y[]{}{}{\irs}{} \le 1 $
border of the image plane.
%The global reference system 
%is only referred to 
%by the expert system (see section)
%which makes navigation decisions based on globally consistent information
%when generating training data for the ANN module.
%The autonomous navigation method, 
%with the fully trained ANN module making the navigation decisions,
%exclusively resorts to data from onboard sensors,
%which is relative to the drone and its onboard camera.
%The method, therefore, operates only within the local and image reference system.
\begin{figure}[h]
    \centering
    \subfloat[
        Local reference system
    ]{
        \label{fig:local_reference_system}
        \includegraphics[width=0.441\textwidth]{own/local_reference_system.pdf}
    }                
    \subfloat[
        Image reference system
    ]{
        \label{fig:image_reference_system}
        \includegraphics[width=0.5\textwidth]{own/image_reference_system.pdf}
    }
    \caption[
        The local and the image reference system
    ]{
        The local and the image reference system. 
        The local reference system (red) is aligned with the drone's onboard camera. 
        The image reference system (blue) is superimposed on the images from the onboard camera.
        The pictured, exemplary waypoint
        $\pos[]{\wayp}{}{\irs}{} = \begin{bmatrix} -0.428 & 0.136 \end{bmatrix}^T$
        (orange) with respect to the image reference system
        is part of the label for the underlying image.
        \label{fig:local_and_image_reference_system}
    }
\end{figure}
%https://www.researchgate.net/figure/Pin-hole-camera-model-terminology-The-optical-center-pinhole-is-placed-at-the-origin_fig10_317498100





\paragraph*{Transformation between the global and the local reference system} $\ $\\
The drone's position
$\pos[]{\drone}{}{\grs}{}$
and quaternion orientation
$\quat[]{\drone}{}{\grs}{}$
with respect to the global reference system
are the parameters that determine the bidirectional transformation
between the global and the local reference system.
The following bases on quaternion mathematics,
for which one can consult, e.g., \cite{Parent}.
A point given in the coordinates of the global reference system
can be expressed in the coordinates of the local reference system
with the transformation
\begin{align} \label{equ:global_to_local_transformation}
    \trafo[]{}{\lrs\grs}{}{}
    :\ 
    \mathbb{R}^3 \rightarrow \mathbb{R}^3
    ; \quad
    \pos[]{}{}{\grs}{} \mapsto \pos[]{}{}{\lrs}{}
    =
    %\begin{cases}
        \mathcal{P} \left[
            \mathrm{inv} \left( \quat[]{\drone}{}{\grs}{} \right)
            *
            \mathcal{Q} \left( \pos[]{}{}{\grs}{} - \pos[]{\drone}{}{\grs}{} \right)
            *
            \quat[]{\drone}{}{\grs}{}
        \right], 
        %& \text{if } \quat[]{\drone}{}{\grs}{} \ne \underline 0 \\
        %\pos[]{}{}{\grs}{} - \pos[]{\drone}{}{\grs}{}, 
        %& \text{else}.
    %\end{cases}
\end{align}
Reversely, a point given in the coordinates of the local reference system
can be expressed in the coordinates of the global reference system
with the transformation
\begin{align} \label{eq:local_to_global_transformation}
    \trafo[]{}{\grs\lrs}{}{}
    :\ 
    \mathbb{R}^3 \rightarrow \mathbb{R}^3
    ; \quad
    \pos[]{}{}{\lrs}{} \mapsto \pos[]{}{}{\grs}{}
    =
    %\begin{cases}
        \mathcal{P} \left[
            \quat[]{\drone}{}{\grs}{}
            *
            \mathcal{Q} \left( \pos[]{}{}{\lrs}{} \right)
            *
            \mathrm{inv} \left( \quat[]{\drone}{}{\grs}{} \right)
        \right] + \pos[]{\drone}{}{\grs}{}, 
        %& \text{if } \quat[]{\drone}{}{\grs}{} \ne \underline 0 \\
        %\pos[]{}{}{\lrs}{} + \pos[]{\drone}{}{\grs}{}, 
        %& \text{else}.
    %\end{cases}
\end{align}
In the two above transformations,
the mapping $\mathcal{Q}$
of a point to its quaternion representation 
and the reverse mapping $\mathcal{P}$ 
of a quaternion representation to its point  
are given by
\begin{align}
    \mathcal{Q}
    :\ 
    & \mathbb{R}^3 \rightarrow \mathbb{R}^4
    ;\
    \pos[]{}{}{}{} = \begin{bmatrix} \x[]{}{}{}{} \\ \y[]{}{}{}{} \\ \z[]{}{}{}{} \end{bmatrix} 
    \mapsto
    \quat[]{}{}{}{} = \begin{bmatrix} w \\ \underline p \end{bmatrix} 
    \text{ with } w = 0
    \nonumber \\
    \mathcal{P}
    :\ 
    &\mathbb{R}^4 \rightarrow \mathbb{R}^3
    ;\
    \quat[]{}{}{}{} = \begin{bmatrix} w \\ \underline p \end{bmatrix} 
    \mapsto
    \pos[]{}{}{}{} = \begin{bmatrix} \x[]{}{}{}{} \\ \y[]{}{}{}{} \\ \z[]{}{}{}{} \end{bmatrix}.
\end{align}
Moreover, the operator $*$ denotes the multiplication of two quaternions which is given by
\begin{align}
    \quat[]{}{1}{}{} * \quat[]{}{2}{}{}
    = 
    \begin{bmatrix}
        w_1 w_2 - \pos[]{T}{1}{}{} \pos[]{}{2}{}{}\\ 
        w_1 \pos[]{}{2}{}{} + w_2 \pos[]{}{1}{}{} + \pos[]{}{1}{}{} \times \pos[]{}{2}{}{}
    \end{bmatrix}.
\end{align}
Finally, the inversion of a quaternion is given by
\begin{align}
    \mathrm{inv}(\quat[]{}{}{}{}) 
    = 
    \frac{1}{\| \quat[]{}{}{}{} \|_2}
    \begin{bmatrix} w \\ \text - \pos[]{}{}{}{} \end{bmatrix}.
\end{align}
The two above transformations are the inversion of each other.
Therefore, points can be transformed between the global and local reference system
without information loss
\begin{equation}
    \trafo[]{}{\grs\lrs}{}{} \circ \trafo[]{}{\lrs\grs}{}{} 
    \left( \pos[]{}{}{\grs}{} \right)
    =
    \pos[]{}{}{\grs}{}
    ,\quad
    \trafo[]{}{\lrs\grs}{}{} \circ \trafo[]{}{\grs\lrs}{}{} 
    \left( \pos[]{}{}{\lrs}{} \right)
    =
    \pos[]{}{}{\lrs}{}.
\end{equation}
In the above equations, the operator $\circ$ denotes the composition of two functions.



\paragraph*{Transformation between the local and the image reference system} $\ $\\
The horizontal
$\ang[\user]{\camera}{\text h}{}{}$
and the vertical
$\ang[\user]{\camera}{\text v}{}{}$
angle of view
of the drone's onboard camera
are the parameters 
that determine the bidirectional transformation 
between the local and the image reference system.
A point given in the coordinates of the local reference system
is expressed in the coordinates of the image reference system with the transformation
\begin{align} \label{equ:local_to_image_transformation}
    \trafo[]{}{\irs\lrs}{}{}
    :\ 
    \mathbb{R}^3 \rightarrow \left[ \text{-}1, 1 \right]^2
    ; \quad
    \pos[]{}{}{\lrs}{} \mapsto \pos[]{}{}{\irs}{}
    =
    \begin{bmatrix}
        \maxof{\text -1}{
            \minof{
                \frac{\text -2}{\ang[\user]{\camera}{\text h}{}{}}
                \mathrm{atan2}\left( \y[]{}{}{\lrs}{}, \x[]{}{}{\lrs}{} \right)
            }{1}
        }
        \\
        \maxof{\text -1}{
            \minof{
                \frac{2}{\ang[\user]{\camera}{\text v}{}{}}
            \mathrm{atan2} \left( \z[]{}{}{\lrs}{}, \| \pos[]{}{}{\lrs}{} \|_2 \right)
            }{1}
        }
    \end{bmatrix}
    .
\end{align}
The above transformation
can be interpreted as the projection of a point onto the image plane 
of the drone's onboard camera.
It can be devided into three steps.
First, the vector from the optical center of the camera 
to the point to be transformed
is mapped to its yaw
$\mathrm{atan2}\left( \y[]{}{}{\lrs}{}, \x[]{}{}{\lrs}{} \right)$
and pitch 
$\mathrm{atan2} \left( \z[]{}{}{\lrs}{}, \| \pos[]{}{}{\lrs}{} \|_2 \right)$
angle, both, with respect to the image reference system.
Second these angles are normalized by 
the half of the horizontal 
$\ang[\user]{\camera}{\text h}{}{}$ 
and the half of the vertical
$\ang[\user]{\camera}{\text v}{}{}$
angle of view of the camera, respectively.
Third, these normalized angles are bounded to be in the interval from minus to plus one.
This boundary takes into account 
that an artifical neural network, which inputs images, 
has no basis for predictions 
that relate to objects that are not within the camera's field of view.
As a projection from 3D to 2D, the above transformation is accompanied by information loss
and is hence not bijective.

A point given in the coordinates of the image reference system is expressed
in the coordinates of the local reference system with the reverse transformation
\begin{align} \label{eq:image_to_local_transformation}
    \trafo[]{}{\lrs\irs}{}{}
    &:\ 
    \mathbb{R}_{\ge 0}, \left[ \text{-}1, 1 \right]^2 \rightarrow \mathbb{R}^3
    ; \quad
    d, \pos[]{}{}{\irs}{} \mapsto \pos[]{}{}{\lrs}{}
    =
    d \begin{bmatrix}
        \cos \left( \ang[]{}{y}{\lrs}{} \right) \\
        \cos \left( \ang[]{}{y}{\lrs}{} \right) \\
        \sin \left( \ang[]{}{y}{\lrs}{} \right)
    \end{bmatrix} \odot \begin{bmatrix}
        \cos \left( \ang[]{}{z}{\lrs}{} \right) \\
        \sin \left( \ang[]{}{z}{\lrs}{} \right) \\
        1
    \end{bmatrix}
    \nonumber \\
    & \qquad \text{with} \quad
    \ang[]{}{z}{\lrs}{}
    = 
    \text - \frac{\ang[\user]{\camera}{\text h}{}{}}{2} \cdot \x[]{}{}{\irs}{}
    ,\quad 
    \ang[]{}{y}{\lrs}{}
    = 
    \frac{\ang[\user]{\camera}{\text v}{}{}}{2} \cdot \y[]{}{}{\irs}{}.
\end{align}
In the above transformation,
the operator 
$\odot$ 
denotes the Hadamard product, 
i.e., the element-wise product of two equally dimensioned matrices.
Because the 2D coordinates of the image reference system
can only contain information about the direction of a point,
the above transformation to 3D requires the additional input of a backprojection length $d$.

In contrast to the transformations 
$\trafo[]{}{\lrs\grs}{}{}$
and 
$\trafo[]{}{\grs\lrs}{}{}$
between the global and the local reference system,
the transformations 
$\trafo[]{}{\irs\lrs}{}{}$
and 
$\trafo[]{}{\lrs\irs}{}{}$
between the local and the image reference system
are not invertible.
However, for relevant points
located within the camera's field of view
and a well chosen backprojection length,
it is assumed that the transformations approximately invert each other
\begin{equation}
    \trafo[]{}{\lrs\irs}{}{} \left[
        d, \trafo[]{}{\irs\lrs}{}{} \left( \pos[]{}{}{\lrs}{} \right)
    \right]
    \approx
    \pos[]{}{}{\lrs}{}
    ,\quad
    \trafo[]{}{\irs\lrs}{}{}
    \circ 
    \trafo[]{}{\lrs\irs}{}{} \left(
        d, \pos[]{}{}{\irs}{}
    \right)
    \approx
    \pos[]{}{}{\irs}{}.
\end{equation}





\paragraph*{Transformation between the global and the image reference system} $\ $\\
The bidirectional transformations of points between the global and the image reference frame
are the compositions of the transformations via the intermittent local reference system
\begin{align} \label{eq:global_image_transformations}
    \trafo[]{}{\irs\grs}{}{}
    &=
    \trafo[]{}{\irs\lrs}{}{} \circ \trafo[]{}{\lrs\grs}{}{}
    :\ 
    \mathbb{R}^3 \rightarrow \left[\text{-} 1, 1\right]^2 
    \nonumber \\
    \trafo[]{}{\grs\irs}{}{}
    &=
    \trafo[]{}{\grs\lrs}{}{} \circ \trafo[]{}{\lrs\irs}{}{}
    :\ 
    \mathbb{R}_{\ge 0}, \left[\text{-} 1, 1\right]^2 \rightarrow \mathbb{R}^3.
\end{align}
Due to the fact that
$\trafo[]{}{\lrs\grs}{}{}$
and
$\trafo[]{}{\grs\lrs}{}{}$
are the inverse of each other
and the assumption that
$\trafo[]{}{\irs\lrs}{}{}$
and
$\trafo[]{}{\lrs\irs}{}{}$
approximately invert each other within a relevant range,
the above compositions are expected to also approximately invert each other within this relevant range
\begin{equation}
    \trafo[]{}{\grs\irs}{}{} \left[
        d, \trafo[]{}{\irs\grs}{}{} \left( \pos[]{}{}{\grs}{} \right)
    \right]
    \approx
    \pos[]{}{}{\grs}{}
    ,\quad
    \trafo[]{}{\irs\grs}{}{}
    \circ 
    \trafo[]{}{\grs\irs}{}{} \left(
        d, \pos[]{}{}{\irs}{}
    \right)
    \approx
    \pos[]{}{}{\irs}{}.
\end{equation}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.9\textwidth]{own/global_via_local_to_image_transformation_2d.pdf}
%    \caption[
%        Schematic 2D depiction
%        of the transformation of the waypoint 
%        from the global via the local to the image reference system.
%    ]{
%        Schematic 2D depiction
%        of the transformation of the waypoint 
%        from the global ${}_\textbf{G}\square$ 
%        via the local ${}_\textbf{L}\square$ 
%        to the image ${}_\textbf{I}\square$ reference system.
%        Unit vectors 
%        $\underline e_\square \in \mathbb{R}^3,\ \left\|\underline e_\square \right\|_2 = 1$ 
%        spanning the individual reference systems,
%        points $\underline p^\square \in \mathbb{R}^3$,
%        quaternions $\underline q^\square \in \mathbb{R}^4$
%        and angles $\phi_\square^\square  \in \mathbb{R}$
%        relative to the global, local or image reference system
%        are colored in green, red or blue, respectively.
%        %Unit vectors are drawn
%        %with a greater linewidth and lower opacity 
%        %than the positions, quaternions and angles.
%        The z unit vectors 
%        ${}_\textbf{G} e^\text{G}_z$
%        and
%        $\ {}_\textbf{L} e^\text{L}_z$ 
%        of the global and local reference system,
%        the y unit vector 
%        ${}_\textbf{I} e^\text{I}_y$ 
%        of the image reference system
%        and the pitch angle 
%        $\phi^\text{user}_{y,\text{camera}}$
%        of view of the camera
%        are not pictured
%        in this 2D representation.
%        %but point in the direction of the reader.
%        The field of view (FOV) of the onboard camera of the drone
%        is marked with a low-opaque orange.
%
%        The global position
%        ${}_\textbf{G}\underline p^\text{drone}$
%        and quaternion orientation
%        ${}_\textbf{G}\underline q^\text{drone}$
%        of the drone,
%        the yaw
%        $\phi^\text{user}_{z,\text{camera}}$
%        and pitch
%        $\phi^\text{user}_{y,\text{camera}}$ 
%        angle of view of the camera
%        as well as the global position of the waypoint
%        ${}_\textbf{G}\underline p^\text{wayp}$
%        are known.
%        First, the local position of the waypoint
%        ${}_\textbf{L}\underline p^\text{wayp}$
%        is computed by applying the transformation
%        $T_\textbf{LG}$
%        (equation \ref{equ:global_to_local_transformation}).
%        Second, 
%        ${}_\textbf{L}\underline p^\text{wayp}$
%        is transformed with 
%        $T_\textbf{IL}$
%        (equation \ref{equ:local_to_image_transformation})
%        yielding 
%        the position of the waypoint
%        ${}_\textbf{I}\underline p^\text{wayp}$ 
%        with respect to the image reference system.
%        $T_\textbf{LG}$ is fully determined by 
%        ${}_\textbf{G}\underline p^\text{drone}$ 
%        and 
%        ${}_\textbf{G}\underline q^\text{drone}$.
%        $T_\textbf{IL}$ is fully determined by
%        $\phi^\text{user}_{z,\text{camera}}$
%        and
%        $\phi^\text{user}_{y,\text{camera}}$.
%        \label{fig:trafo}
%    }
%\end{figure}



\section{ANN module} \label{sec:ann_module}
The ANN module performs the function of making navigation decisions
within the autonomous navigation method.
It infers these decisions exclusively 
on the basis of data from the drone's onboard sensors.


%In order for these navigation decisions to be reasonable
%the ANN module is trained with supervised learning (see section ??).
%At testing, 
%when the autonomous navigation method flies the drone through the racetrack,
%the ANN module outputs navigation decisions in real-time at a user-specified frequency.
%Thereby, the GRU sub-module (and therewith the ANN module itself)
%operates in one-to-one mode,
%i.e., it maps each single, non-sequential input to a single, non-sequential output.
%As, however, the frequently incoming, single inputs as a whole constitute a time series,
%the GRU sub-module builds up memory in its hidden state
%as the module learned during training.
%By this, not only the current but also 
%past inputs have impact on the current navigation decision.


The ANN module comprises
the CNN, the CAT, the GRU, the FC and the HEAD sub-module
(see figure XXX).
This modular design entails a high flexibility for the experiments
with the different ANN module variants in chapter \ref{maintwo}.
All variants have in common that,
first, the order of the sub-modules is fixed
and, second, the outer sub-modules, i.e, the CNN and the HEAD sub-module,
are always activated.
The latter ensures the minimum functionality of 
mapping the minimum input, i.e., RGB images,
to navigation decisions.
The variants differ in that
the user specifies the design parameters of 
all, the inner and outer, individual sub-modules.
This includes, whether individual inner sub-modules,
i.e., the CAT, the GRU and the FC sub-module,
are activated.


\paragraph*{Input} ${}$\\
The input of the ANN module is devided into mandatory and optional.
The mandatory input for an inference step is the
latest, preprocessed RGB (therewith three channeled) image from the drone's onboard camera
\begin{equation}
    \underline{\underline{\underline I}}^\text{preproc}
    \in 
    %\left\{
    %    0,\dots,I_\text{max}
    %\right\}
    \left\{
        \frac{i}{I_\text{max}^\text{raw}}
    \right\}
    _{
        i \in 
        \left\{
            0,\dots,I_\text{max}^\text{raw}
        \right\}
    }
    ^{
        %I_\text{C} \times    
        3 \times
        I_\text{H}^\text{preproc} \times 
        I_\text{W}^\text{preproc}
    }.
    %\text{ with } I_\text{C} = 3.
\end{equation}
The two-step preprocessing of the raw RGB image simplifies the training of the ANN module.
First, the pixel intensities are normalized by the full intensity 
$I_\text{max}^\text{raw} = 255$
with the aim to accelerate the convergence at training.
Second, the image is sized down 
to the height 
$
    I_\text{H}^\text{preproc} 
    = 
    \left\lfloor
    s^\text{user}_\text{rgb-resize}
    \cdot
    I_\text{H}^\text{raw}
    \right\rfloor
$
and width
$
    I_\text{W}^\text{preproc} 
    = 
    \left\lfloor
    s^\text{user}_\text{rgb-resize}
    \cdot
    I_\text{W}^\text{raw}
    \right\rfloor
$
with a user-specified 
factor preserving its aspect ratio.
By this, the occupation of GPU memory at training,
which is critical for longer sequences,
can be reduced if necessary.



Before the training of an ANN module variant, the user switches on or off
the individual elements of the feature vector of optional inputs
\begin{align}
    \featvec[]{\text{opt}}{}{}{} 
    =
    \begin{bmatrix}
        \dur[]{\text{rgb}}{}{}{} \\
        \acc[\hat]{\text{imu}}{}{}{} \\
        \angvel[\hat]{\text{imu}}{}{}{} \\
        \dur[]{\text{imu}}{}{}{}
    \end{bmatrix}
    .
\end{align}
The above vector comprises the
the duration
$\dur[]{\text{rgb}}{}{}{}$
elapsed between the shooting of the previous and the latest raw RGB image,
the drone's latest linear acceleration
$\acc[\hat]{\text{imu}}{}{}{} \in \mathbb{R}^3$
and angular velocity
$\angvel[\hat]{\text{imu}}{}{}{} \in \mathbb{R}^3$
measurement from its onboard inertial measurement unit (IMU)
as well as the duration
$\dur[]{\text{imu}}{}{}{}$
elapsed between the recording of the previous and the latest IMU data.







\paragraph*{Input preprocessing} ${}$\\
The drone's onboard camera 
outputs raw RGB images
\begin{equation}
    \img[]{\camera}{}{}{}
    \in 
    \left\{ 0, \dots, \num[]{\camera}{\mxm}{}{} \right\}^{
        3 \times
        \num[]{\camera}{\height}{}{}
        \times 
        \num[]{\camera}{\widthh}{}{}
    }
\end{equation}
with the full pixel intensity $\num[]{\camera}{\mxm}{}{}$ (usually $= 255$)
and  the height $\num[]{\camera}{\height}{}{}$ 
and width $\num[]{\camera}{\widthh}{}{}$.
Before being fed to the CNN sub-module,
the raw RGB images are preprocessed in two steps.
\begin{enumerate}
    \item Normalize the pixel intensities by the full intensity 
    $\num[]{\camera}{\mxm}{}{}$. 
    This step aims at an accelerated convergence of the training loss.
    \item Size the image down 
    preserving its aspect ratio
    with the user-specified factor
    $\anything[\user]{\cnn}{\text{resize}}{}{}{s}$.
    %to the height and width,
    %while preserving the aspect ratio
    %\begin{equation}
    %    \num[]{\cnn}{\height}{}{}
    %    = 
    %    \left\lfloor 
    %        \anything[\user]{\cnn}{\text{resize}}{}{}{s}
    %        \cdot
    %        \num[]{\camera}{\height}{}{}
    %    \right\rfloor
    %    ,\quad 
    %    \num[]{\cnn}{\widthh}{}{}
    %    = 
    %    \left\lfloor 
    %        \anything[\user]{\cnn}{\text{resize}}{}{}{s}
    %        \cdot
    %        \num[]{\camera}{\widthh}{}{}
    %    \right\rfloor.
    %\end{equation}
    Besides significantly accelerating the training,
    this step makes training on longer sequences possible
    in the first place by reducing GPU memory usage.
\end{enumerate}
Therewith, the preprocessed RGB image,
which is ready to be put into the CNN sub-module, 
is
\begin{equation} \label{eq:rgb_preproc}
    \img[]{\cnn}{}{}{}
    \in 
    \left\{ \frac{i}{\num[]{\camera}{\mxm}{}{}} \right\}_{
        i \in \left\{0, \dots, \num[]{\camera}{\mxm}{}{}\right\}
    }^{
        3 \times
        \left\lfloor 
            \anything[\user]{\cnn}{\text{resize}}{}{}{s}
            \cdot
            \num[]{\camera}{\height}{}{}
        \right\rfloor
        \times 
        \left\lfloor 
            \anything[\user]{\cnn}{\text{resize}}{}{}{s}
            \cdot
            \num[]{\camera}{\widthh}{}{}
        \right\rfloor
    }
\end{equation} 
where $\lfloor\ \rfloor$ means round down to the nearest integer.


The optional inputs, that are activated by the user,
are stacked into a vector. The fully activated,
optional input vector is
\begin{align}
    \featvec[]{\text{opt}}{}{}{} 
    =
    \begin{bmatrix}
        \dur[]{\text{rgb}}{}{}{} \\
        \acc[\hat]{\text{imu}}{}{}{} \\
        \angvel[\hat]{\text{imu}}{}{}{} \\
        \dur[]{\text{imu}}{}{}{}
    \end{bmatrix}
\end{align}
with the time step
$\dur[]{\text{rgb}}{}{}{}$
between the time stamps of the previously and the currently inputted RGB image,
the latest linear acceleration
$\acc[\hat]{\text{imu}}{}{}{} \in \mathbb{R}^3$
and angular velocity
$\angvel[\hat]{\text{imu}}{}{}{} \in \mathbb{R}^3$
estimate of the drone's onboard inertial measurement unit (IMU)
as well as the time step
$\dur[]{\text{imu}}{}{}{}$
between the time stamps of the
previously and the currently inputted IMU estimate.








\paragraph*{CNN} ${}$\\
The convolutional neural network (CNN) sub-module 
extracts visual features 
of the preprocessed RGB images 
(equ. \ref{eq:rgb_preproc})
from the drone's onboard camera.
This sub-module is implemented as the backbone
of the TorchVision\footnote{
    \url{https://pytorch.org/vision/stable/models.html}, visited on 23/08/2022
}
classification model specified by the user.
The backbone is obtained by simply removing the last layer of the model.
Since CNN backbones are only applied in this thesis, 
their corresponding mapping is 
regarded as a black box 
\begin{align} \label{eq:CNN}
    \mathcal{F}^\text{cnn}_{\text{backbone}}
    &:
    \mathbb{R}^{
        \num[]{\cnn}{\channel}{}{}
        \times
        \num[]{\cnn}{\height}{}{}
        \times
        \num[]{\cnn}{\widthh}{}{}
    } 
    \rightarrow 
    \mathbb{R}^{
        \num[]{\cnn}{\out}{}{}
    }
    ;\quad
    %\nonumber \\ & \quad
    \img[]{}{}{}{}
    \mapsto 
    \mathcal{F}^\cnn \left(\img[]{}{}{}{}\right)
    .
\end{align}
The backbone implementation adapts to the inputted image
in terms of the 
the number of channels $\num[]{\cnn}{\channel}{}{}$,
height $\num[]{\cnn}{\height}{}{}$
and width $\num[]{\cnn}{\widthh}{}{}$.
In contrast, the backbone's output size $\num[]{\cnn}{\out}{}{}$
is fixed by its design.
At training, the input to the CNN backbone is
a batch of sequences of preprocessed RGB images
\begin{equation}
    \left(
        \img[]{\cnn}{t}{}{}
    \right)_{
        t \in \left\{1, \dots, \num[\user]{\ann}{\seq}{}{}\right\}
        , i
    }
    , \quad 
    i \in \left\{1, \dots, \num[\user]{\ann}{\batch}{}{}\right\}.
\end{equation}
with the user-specified
batch size $\num[\user]{\ann}{\batch}{}{}$
and sequence length $\num[\user]{\ann}{\seq}{}{}$.
At racing, both equate one.
The CNN backbone,
which treats sequence elements just as batch elements,
maps each processed RGB image of the input batch
to an individual vector of visual features
\begin{equation}
    \left(
        \mathcal{F}^\text{cnn} \left(
            \img[]{\cnn}{t}{}{}
        \right)
    \right)_{
        t \in \left\{1, \dots, \num[\user]{\ann}{\seq}{}{}\right\}
        , i
    }
    , \quad 
    i \in \left\{1, \dots, \num[\user]{\ann}{\batch}{}{}\right\}.
\end{equation}
Hence, the CNN sub-module 
outputs a batch of sequences of visual feature vectors.




\paragraph*{CAT} ${}$\\
The CAT sub-module simply concatenates
two feature vectors
\begin{align} \label{eq:cat}
    \mathcal{F}^\text{\cat}
    &:
    \left(
        \mathbb{R}^{\num[]{\cat}{1}{}{}},
        \mathbb{R}^{\num[]{\cat}{2}{}{}}
    \right)
    \rightarrow 
    \mathbb{R}^{
        \num[]{\cat}{1}{}{} + 
        \num[]{\cat}{2}{}{}
    }
    ;\quad
    %\nonumber \\ &
    \left(
        \featvec[]{}{1}{}{}, 
        \featvec[]{}{2}{}{}
    \right)
    \mapsto
    \begin{bmatrix}
        \featvec[]{}{1}{}{} \\ \featvec[]{}{2}{}{}
    \end{bmatrix}.
\end{align}
with
$\mathbb{R}^{\num[]{\cat}{1}{}{}}$
and
$\mathbb{R}^{\num[]{\cat}{2}{}{}}$
adapting to the dimensionalities of the input.
This sub-module applies to each
visual feature vector
outputted by the CNN sub-module 
and optional input feature vector
that correspond to the same position in batch and sequence
\begin{equation}
    \left(
        \mathcal{F}^\text{\cat} \left(
            \mathcal{F}^\text{cnn} \left(
                \img[]{\cnn}{t}{}{}
            \right),
            \featvec[]{\text{opt}}{t}{}{}
        \right)
    \right)_{
        t \in \left\{1, \dots, \num[\user]{\ann}{\seq}{}{}\right\}
        , i
    }
    , \quad 
    i \in \left\{1, \dots, \num[\user]{\ann}{\batch}{}{}\right\}.
\end{equation}
Hence the CAT sub-module outputs a batch of sequences
of concatenated feature vectors.
Note that, if the user deactivates all optional input
the CAT sub-module recedes to 
the identity map of the the CNN sub-module output.
Either way, 
the CAT sub-module has zero trainable parameters
\begin{equation}
    N^\text{cat}_\text{param} = 0.
\end{equation}


\subsection*{GRU}
The GRU (gated recurrent unit) submodule
is implemented using the PyTorch
multi-layer GRU\footnote{
    \url{https://pytorch.org/docs/stable/generated/torch.nn.GRU.html}, visited on 24/08/2022
},
which, implements the GRU \cite{Cho2014} 
introduced in section \ref{sec:gru} on each layer.
As a quick reminder, 
the GRU has the ability to comprehend temporal relations within sequences.
The single GRU layer processes a sequence by
iterating through it
mapping the current sequence element $\underline x_t$ and the previous hidden state 
$\underline h_{t-1}$ 
to the current hidden state (see equ. \ref{eq:gru_layer_current_hidden})
\begin{align}
    h_{t}
    =
    \mathcal{F}^\text{h} \left( \underline x_{t}, \underline h_{t-1}\right)
\end{align}
with $\underline h_{0}$ initialized.
In the multi-layer GRU,
each layer recurrently maintains its own hidden state.
In addition to their corresponding hidden state,
the first GRU layer inputs the elements of the given sequence
while all subsequent layers input the hidden state,
subject to dropout,
from the previous layer.
The output of the whole multi-layer GRU
is the hidden state of its last layer.
The multi-layer GRU mapping can be 
formulated recursively
\begin{align} \label{eq:gru}
    \mathcal{F}^\text{\gru}
    &:
    \left(
        \mathbb{R}^{\num[]{\gru}{\text{in}}{}{}},
        \left[-1, 1\right]^{\num[]{\gru}{\text{hidden}}{}{} \times \num[]{\gru}{\text{layer}}{}{}}
    \right)
    \rightarrow 
    \mathbb{R}^{
        \num[]{\gru}{\text{hidden}}{}{}
    }
    \nonumber \\
    %\left( 
    %    \underline x_{t}, 
    %    \underline h_{t-1, 1}, 
    %    \dots, 
    %    \underline h_{t-1, \num[]{\gru}{\text{layer}}{}{}}
    %\right)
    %\mapsto
    \underline h_{t, \num[]{\gru}{\text{layer}}{}{}} 
    &=
    \mathcal{F}^{\gru} \left( 
        \underline x_{t}, 
        \underline h_{t-1, 1}, 
        \dots, 
        \underline h_{t-1, \num[]{\gru}{\text{layer}}{}{}}
        \right)
    \nonumber \\
    &= 
    \mathcal{F}^\text{h} \left( 
        \underline\delta^\gru
        \odot
        \underline h_{t, \num[]{\gru}{\text{layer}}{}{}-1}
    , \underline h_{t-1,\num[]{\gru}{\text{layer}}{}{}}\right)
    ,\quad \underline h_{t, 0} = \featvec[]{}{t}{}{}
    .
\end{align}
with all hidden states initialized.
%The above mapping ultimately depends on
%\begin{align}
%    \mathcal{F}^{\gru} \left( 
%        \left(\underline x_{t}\right)_{
%            \left\{1, \dots, \num[]{\seq}{}{}{}\right\}
%        },
%        \underline h_{t-1, 1}, 
%        \dots, 
%        \underline h_{t-1, \num[]{\gru}{\text{layer}}{}{}}
%    \right)
%    .
%\end{align}
Dropout \cite{Hinton2012} decreases the overfitting of an ANN 
on the provided training data
by enforcing the neurons to learn the detection of stand-alone features
whose informative value is independent from
the relation to other detected features.
For this purpose, dropout randomly sets entries of
a feature vector to zero while maintaining the signal strength on average.
The dropout is accomplished
by the Hadamard product (denoted with $\odot$)
with the vector
\begin{equation}
    \underline{\delta}^\gru
    =
    \left[
        \delta^\gru_i
    \right]_{
        i \in \left\{
            1, \dots, \num[]{\gru}{\text{hidden}}{}{}
        \right\}
    }
\end{equation}
of independent, scaled Bernoulli random variables
that are resampled for every calculated Hadamard product
with the probabilities
\begin{equation}
    P \left(
        \delta^\gru_i = 0
    \right)
    = p^\gru
    ,\quad
    P \left(
        \delta^\gru_i = \frac{1}{1-p^\gru}
    \right)
    = 1 - p^\gru
    .
\end{equation}
During training,
the dropout probability $p^\gru$ equates to 
a user-specified value $\anything[\user]{\gru}{}{}{}{p} \in [0,1]$,
whereas during racing,
it is null
whereby the dropout becomes an identity operation.

The GRU submodule 
processes each sequence of concatenated feature vectors
in the batch outputted by the CAT submodule.
At training, the GRU submodule operates in many-to-one mode
as it inputs sequences and outputs only the temporally last hidden state. 
At racing, the GRU submodule operates in one-to-one mode
as every incoming single input is processed.
Either way, the output of the GRU submodule is
\begin{equation}
    \underline h_{
        \num[\user]{\ann}{\seq}{}{}, 
        \num[\user]{\gru}{\text{layer}}{}{},
        i
    } 
    , \quad 
    i \in \left\{1, \dots, \num[\user]{\ann}{\batch}{}{}\right\}
    .
\end{equation}




\subsection*{FC}
The FC submodule comprises multiple fully connected layers.
The function of this submodule is to 
generally increase the complexity of the ANN module.
Each layer 
$
    l 
    \in 
    \left\{
        1, \dots, \num[\user]{\fc}{\layer}{}{}
    \right\}
$ 
applies
an activation function,
a dropout
and a biased linear transformation
on the feature vector inputted
\begin{align} \label{eq:fc_layer}
    \mathcal{F}^\text{fc}_l
    :\ &
    \mathbb{R}^{
        \num[]{\text{fc}}{\text{in}, l}{}{}
    }
    \rightarrow 
    \mathbb{R}^{
        \num[\user]{\text{fc}}{\text{out}, l}{}{}
        }
    \nonumber \\ &
    \underline{x}
    \mapsto
    \underline{\underline A}^\text{fc}_l
    \left(
        \underline{\delta}^\text{fc}
        \odot
        \overset{\scriptscriptstyle \odot}{\func[\user]{\fc}{}{}{}} \left(
            \underline{x}
        \right)
    \right)
    + \underline b^\text{fc}_l
    .
\end{align}
The mapping of the whole FC submodule is recursively defined
\begin{align} \label{eq:fc}
    \mathcal{F}^\text{fc}
    :\ &
    \mathbb{R}^{
        \num[]{\text{fc}}{\text{in}, 1}{}{}
    }
    \rightarrow 
    \mathbb{R}^{
        \num[\user]{\text{fc}}{\text{out}, l}{}{}
        }
    \nonumber \\ &
    \featvec[]{}{}{}{}
    \mapsto
    \mathcal{F}^\text{fc}_{\num[\user]{\fc}{\layer}{}{}} \left(
        \mathcal{F}^\text{fc}_{\num[\user]{\fc}{\layer}{}{} - 1} \left(
            \dots
            \mathcal{F}^\text{fc}_{1} \left(
                \featvec[]{}{}{}{}
            \right)    
        \right)
    \right)
    .
\end{align}
For the FC submodule,
the user specifies:
\begin{itemize}
    \item the activation function
    $\func[\user]{\fc}{}{}{}: \mathbb{R} \rightarrow \mathbb{R} $
    from the non-linear activations
    implemented by PyTorch\footnote{
        \url{https://pytorch.org/docs/stable/nn.html}, visited on 03/07/2022
    },
    which applies element-wise 
    on the input (denoted with the overset ${}^\odot$)
    \item the dropout probability 
    $\prob[\user]{\fc}{}{}{} \in [0,1]$,
    i.e., the probability that an entry of the vector 
    $\underline{\delta}^\fc$ will be sampled with zero
    \item the number of layers
    $
    \num[\user]{\fc}{\layer}{}{}
    $
    \item the width 
    $
    \num[\user]{\fc}{\outp}{}{}
    $,
    i.e., output dimensionality
    shared by all layers.
\end{itemize}
The input dimensionality of a layer
adapts to the given input vector.
For the first layer,
it adapts to the feature vector
forwarded to the FC submodule
$
\num[]{\fc}{\inp,1}{}{} 
= 
\dim \left(\featvec[]{}{}{}{}\right)$.
For all subsequent layers $l \ge 2$,
it adapts to
the width of the FC submodule
$
\num[]{\fc}{\inp,l}{}{} 
= \num[]{\fc}{\outp}{}{}
$.
The biased linear transformation of the $l$-th layer
consists of the multiplication with the matrix of trainable weights
and the addition of the vector of trainable biases
\begin{equation}
    \underline{\underline A}^\fc_l
    \in 
    \mathbb{R}^{
        \num[]{\fc}{\inp,l}{}{}
        \times
        \num[]{\fc}{\outp}{}{}
    }, \quad
    \underline b^\text{fc}_l 
    \in \mathbb{R}^{
        \num[]{\fc}{\outp}{}{}
    }.
\end{equation}
As a single layer therewith has
$
\left( \num[]{\fc}{\inp,l}{}{} + 1 \right)
\num[]{\fc}{\outp}{}{}
$
trainable parameters,
the total number of trainable parameters of the FC submodule is
\begin{align}
    \num[]{\fc}{\params}{}{}
    = \begin{cases}
        \left( \num[]{\fc}{\inp,1}{}{} + 1 \right)
        \num[\user]{\fc}{\outp}{}{}
        ,& \text{ if } \num[\user]{\fc}{\layer}{}{} = 1
        \\
        \left( \num[]{\fc}{\inp,1}{}{} + 1 \right)
        \num[\user]{\fc}{\outp}{}{}
        +
        \left( \num[\user]{\fc}{\layer}{}{} - 1 \right)
        \left( \num[\user]{\fc}{\outp}{}{} + 1 \right)
        \num[\user]{\fc}{\outp}{}{}
        ,& \text{ else.}
    \end{cases}
\end{align}
Depending on the user specification,
the FC submodule applies 
on the batch of visual feature vectors,
concatenated feature vectors
or hidden states
outputted by the CNN, the CAT or the GRU submodule, respectively.
In general, the submodule applies 
to a batch of single non-sequential feature vectors $\featvec[]{}{i}{}{}$
and 
hence outputs the batch
\begin{equation}
    \mathcal{F}^\text{fc}\left(
        \featvec[]{}{}{}{}
    \right)_i
    , \quad 
    i \in \left\{1, \dots, \num[\user]{\ann}{\batch}{}{}\right\}
    .
\end{equation}



\subsection*{HEAD}
The mandatory HEAD submodule
ensures the mapping 
to the final output of the ANN module,
which is,
depending on the user specification,
either a navigation decision
or a control command.
A navigation decision 
\begin{equation} \label{eq:navigation_decision}
    (
        \speed[\norm]{\drone}{\desired}{}{}
        ,\ 
        \pos[]{\wayp}{}{\irs}{}
    )
\end{equation}
consists of a normalized desired speed 
$\speed[\norm]{\drone}{\desired}{}{} \in [0, 1]$
and a waypoint
$\pos[]{\wayp}{}{\irs}{} \in [-1, 1]^2$
in the image reference system
(see fig. \ref{fig:image_reference_system}).
A control command 
\begin{equation}
    (
        \angvel[]{\drone}{\desired}{\lrs}{},\ 
        \angvel[\dot]{\drone}{\desired}{\lrs}{},\ 
        \anything[]{\drone}{\desired}{}{}{c}
    )
\end{equation}
comprises the desired angular velocity
$\angvel[]{\drone}{\desired}{\lrs}{}$
(also referred to as body rates)
and acceleration
$\angvel[\dot]{\drone}{\desired}{\lrs}{}$
of the drone in the local reference system 
(see fig. \ref{fig:local_reference_system})
as well as the desired collective thrust 
$\anything[]{\drone}{\desired}{}{}{c}$
of the drone rotors.
In the limited scope of this master's thesis,
the option to output control commands,
which is a shortcut to the position controller output 
(see section \ref{sec:control_stack}),
is only partly implemented and not further investigated.

The head sub-module applies 
an activation function 
and a biased linear transformation
\begin{align} \label{eq:head}
    \mathcal{F}^\head
    :\ &
    \mathbb{R}^{ \num[]{\head}{\inp}{}{} }
    \rightarrow 
    \mathbb{R}^{ \num[]{\head}{\outp}{}{} }
    %\nonumber \\ &
    ;\quad 
    \featvec[]{}{}{}{}
    \mapsto
    \underline{\underline A}^\head
    \left(
        \overset{\scriptscriptstyle \odot}{\func[\user]{\head}{}{}{}}
        \left(\featvec[]{}{}{}{}\right)
    \right)
    + \underline b^\head
    ,
\end{align}
which is equal to to a layer of the FC sub-module
(see equ. \ref{eq:fc_layer}) without dropout.
The elementwise applied activation function
$\overset{\scriptscriptstyle \odot}{\func[\user]{\head}{}{}{}}$ 
is, again, specified by the user.
The input dimensionality adapts to the
number of features in the given input
\begin{equation}
    \num[]{\head}{\inp}{}{}
    =
    \dim \left(\featvec[]{}{}{}{}\right),
\end{equation}
while the number of output features is determined by the user-specified output
\begin{equation}
    \num[]{\head}{\outp}{}{}
    = 
    \begin{cases}
        3
        ,\quad 
        \text{if navigation decision} 
        \\
        7
        ,\quad 
        \text{if control command.} 
    \end{cases}
\end{equation}
The total number of trainable parameters of the HEAD sub-module is
\begin{equation}
    \num[]{\head}{\params}{}{}
    = 
    \left( \num[]{\head}{\inp}{}{} + 1 \right)
    \num[]{\head}{\outp}{}{}
    .
\end{equation}






%CNN: torch vision model, pretrained, trainable, adapt to any RGB size
%CAT: CNN output , CAT input , one vector
%GRU: hidden size, num_layers, bias, dropout
%FC: width, num_layers, act fct, bias
%HEAD: output size, act fct, bias
%GRAPH
%
%loss optimizer learnrate
\subsection*{Output}










\section{Planning module}
The planning module performs the task of path planning
within the autonomous navigation method.
At the user-specified main frequency 
$\freq[\user]{\main}{}{}{}$,
the planning module 
samples states from its local trajectory
and forwards them as reference to the control module.
Every $\num[\user]{\text{plan}}{}{}{}$-th (user-specified) iteration,
the planning module re-computes its local trajectory
on the basis of its input,
i.e., the latest navigation decision and the latest drone state estimate. 
%These trajectories, generated by the planning module, 
%are referred to as local trajectories
%to distinguish them from the expert system's global trajectory
%used when generating training data for the ANN module (see XX).

The latest navigation decision
stems from either the ANN module
(see equ. \ref{eq:navigation_decision}) 
or, if it has intervened at training data generation,
the expert system (see equ. \ref{eq:nav_dec_by_expert}).
A navigation decision comprises the normalized desired speed 
and the waypoint in the image reference system
\begin{equation}
    (
        \speed[\norm]{\drone}{\desired}{}{}
        ,\ 
        \pos[]{\wayp}{}{\irs}{}
    ).
\end{equation}
The latest drone state estimate stems from the state estimation system.
In simulation, the estimate may correspond to the ground-truth state.
A drone state estimate includes position, velocity and acceleration
with respect to the global reference system
\begin{equation} \label{eq:drone_state}
    \pos[]{\drone}{}{\grs}{}
    ,\ 
    \vel[]{\drone}{}{\grs}{}
    ,\ 
    \acc[]{\drone}{}{\grs}{}.
\end{equation}


At a fraction of the main frequency, i.e.,
$\freq[\user]{\main}{}{}{} / \num[\user]{\text{plan}}{}{}{}$, 
the planning module takes the following 5 steps 
to re-compute its local trajectory.
\begin{enumerate}
    \item Compute the desired speed
    \begin{equation}
        \speed[]{\drone}{\desired}{}{}
        = 
        \maxof{
            \speed[\user]{\drone}{\mnm}{}{}
        }{
            \speed[\user]{\drone}{\mxm}{}{}
            \cdot 
            \speed[\norm]{\drone}{\desired}{}{}
        }.
    \end{equation}
    The normalized, desired speed 
    $\speed[\norm]{\drone}{\desired}{}{} \in \left[0, 1\right]$ 
    of the navigation decision
    is rescaled by its upper bound, 
    the user-specified drone's maximum speed 
    $\speed[\user]{\drone}{\mxm}{}{}$.    
    The user-specified drone's minimum speed 
    $\speed[\user]{\drone}{\mnm}{}{}$
    lower-bounds the desired speed.

    \item Compute the drone's distance to the waypoint 
    \begin{equation}
        \dist[]{\dronetowayp}{}{}{}
        = 
        \maxof{
            \dist[\user]{\dronetowayp}{\mnm}{}{}
        }{
            \minof{
                \speed[]{\drone}{\desired}{}{}
                \cdot
                \dur[\user]{\dronetowayp}{}{}{}
            }{
                \dist[\user]{\dronetowayp}{\mxm}{}{}
            }
        }.
    \end{equation}
    The desired speed 
    $\speed[]{\drone}{\desired}{}{}$ 
    is integrated over the user-specified duration
    $\dur[\user]{\dronetowayp}{}{}{}$.
    The result is bounded to the interval spanned 
    by the user-specified minimum
    $\dist[\user]{\dronetowayp}{\mnm}{}{}$
    and maximum
    $\dist[\user]{\dronetowayp}{\mxm}{}{}$
    distance.

    \item Compute the waypoint with respect to the global reference system
    \begin{equation} \label{eq:pl_global_wayp}
        \pos[]{\wayp}{}{\grs}{}
        = 
        \trafo[]{}{\grs\irs}{}{} \left(
            \dist[]{\dronetowayp}{}{}{}
            ,\ 
            \pos[]{\wayp}{}{\irs}{}
        \right).
    \end{equation}
    The transformation
    $\trafo[]{}{\grs\irs}{}{}$
    (see equ. \ref{eq:global_image_transformations})
    backprojects the waypoint
    $\pos[]{\wayp}{}{\irs}{}$ 
    of the navigation decision
    from the 2D image to the 3D global reference system.
    Thereby, the drone's distance 
    $\dist[]{\dronetowayp}{}{}{}$
    to the waypoint
    constitutes the backprojection length.
    
    \item Set the starting time of the local trajectory to the current time
    \begin{equation}
        \timepnt[]{\loctraj}{0}{}{} = t
    \end{equation}
    and compute the duration of the local trajectory 
    \begin{equation}
        \dur[]{\loctraj}{}{}{}
        = 
        \frac{
            \dist[]{\dronetowayp}{}{}{}
        }{
            \minof{
            \speed[]{\drone}{\desired}{}{}
            }{
                %\speed[]{\drone}{}{}{} 
                \left\|
                    \vel[]{\drone}{}{\grs}{}
                \right\|_2
                + 
                \speed[\user]{\drone}{\Delta}{}{}
            }
        }.
    \end{equation}
    The drone's distance 
    $\dist[]{\dronetowayp}{}{}{}$
    to the waypoint is devided 
    by the slower of either the desired speed
    $\speed[]{\drone}{\desired}{}{}$
    or the latest drone speed estimate
    $\left\|\vel[]{\drone}{}{\grs}{}\right\|_2$
    plus a user-specified speed increment
    $\speed[\user]{\drone}{\Delta}{}{}$.
    By relating the desired to the estimated speed,
    excessive speed increases
    potentially violating the drone's dynamic limitations
    can be prevented.

    \item Compute the local trajectory 
    \begin{align} \label{eq:loc_traj}
        \pos[]{\loctraj}{}{\grs}{}
        :\ 
        &\left[0, \dur[]{\loctraj}{}{}{}\right] \rightarrow \mathbb{R}^3
        ;\quad
        %\nonumber \\
        \timepnt[]{}{}{}{}
        \mapsto
        \pos[]{\loctraj}{}{\grs}{}(\timepnt[]{}{}{}{})
    \end{align}
    starting in the latest drone state estimate
    $\pos[]{\drone}{}{\grs}{}
    ,\ 
    \vel[]{\drone}{}{\grs}{}
    ,\ 
    \acc[]{\drone}{}{\grs}{}$
    and ending in the global waypoint
    $\pos[]{\wayp}{}{\grs}{}$
    with unconstrained velocity and acceleration.
    The implementation\footnote{
            \url{https://github.com/markwmuller/RapidQuadrocopterTrajectories}, visited on 17/08/2022
    } 
    of the algorithm of Mueller et. al. \cite{Mueller2013}
    is deployed to find the polynomial trajectory with minimum jerk
    (third time derivative of position)
    by solving the optimization problem
    \begin{align}
        &\qquad \min 
        \int_0^{\dur[]{\loctraj}{}{}{}}
            \left\|
                \pos[\dddot]{\loctraj}{}{\grs}{}(\timepnt[]{}{}{}{})
            \right\|^2_2
        \text d \timepnt[]{}{}{}{}
        \nonumber \\
        \text{s.t.}\quad
        & \pos[]{\loctraj}{}{\grs}{}(0) = \pos[]{\drone}{}{\grs}{}
        \qquad\qquad \pos[]{\loctraj}{}{\grs}{}(\dur[]{\loctraj}{}{}{}) = \pos[]{\wayp}{}{\grs}{}
        \nonumber \\
        & \pos[\dot]{\loctraj}{}{\grs}{}(0) = \vel[]{\drone}{}{\grs}{}
        \qquad\qquad \pos[\dot]{\loctraj}{}{\grs}{}(\dur[]{\loctraj}{}{}{}) \text{ free}
        \nonumber \\
        & \pos[\ddot]{\loctraj}{}{\grs}{}(0) = \acc[]{\drone}{}{\grs}{}
        \qquad\qquad \pos[\ddot]{\loctraj}{}{\grs}{}(\dur[]{\loctraj}{}{}{}) \text{ free}.
    \end{align}
    The drone's dynamic limitations
    are only taken into account in subsequent feasibility checks 
    and are exempt from the above optimization problem.
    This allows the algorithm to solve the optimization problem in closed form
    which is characterized by low computational effort.
    The algorithm therewith qualifies 
    to run at the relatively high frequencies
    required by the autonomous navigation method.
\end{enumerate}


At the main frequency $\freq[\user]{\main}{}{}{}$, 
the planning module takes the following 2 steps to sample 
a reference state from its local trajectory.
\begin{enumerate}
    \item Compute the time point of the reference state
    \begin{equation}
        \timepnt[]{\loctraj}{}{}{} 
        = 
        t - \timepnt[]{\loctraj}{0}{}{} + 1/\freq[\user]{\main}{}{}{}.
    \end{equation}
    The actual time $t$ is related to the starting time 
    $\timepnt[]{\loctraj}{0}{}{}$
    of the local trajectory,
    whose time domain starts at zero.
    The addition of the main period
    $1/\freq[\user]{\main}{}{}{}$
    ensures that the sampled reference state remains prospective at all times.
    \item Sample the current reference state from the local trajectory
    \begin{align} \label{eq:ref_state}
        \pos[]{\text{ref}}{}{\grs}{} 
        &= 
        \pos[]{\loctraj}{}{\grs}{}(\timepnt[]{\loctraj}{}{}{})
        \nonumber \\
        \vel[]{\text{ref}}{}{\grs}{} 
        &= 
        \pos[\dot]{\loctraj}{}{\grs}{}(\timepnt[]{\loctraj}{}{}{})
        \nonumber \\
        \acc[]{\text{ref}}{}{\grs}{} 
        &= 
        \pos[\ddot]{\loctraj}{}{\grs}{}(\timepnt[]{\loctraj}{}{}{})
        \nonumber \\
        \jerk[]{\text{ref}}{}{\grs}{} 
        &= 
        \pos[\dddot]{\loctraj}{}{\grs}{}(\timepnt[]{\loctraj}{}{}{})
        \nonumber \\
        \ang[]{\text{ref}}{z}{}{}
        &=
        \mathrm{atan2}\left(
            \speed[]{\text{ref}}{y}{\grs}{}
            ,
            \speed[]{\text{ref}}{x}{\grs}{}
        \right).
    \end{align}
    The reference yaw $\ang[]{\text{ref}}{z}{}{}$ is set so that
    the drone and therewith its onboard camera 
    point in the direction of flight movement.
\end{enumerate}











\section{Control stack} \label{sec:control_stack}
Within the autonomous navigation method, 
the control stack takes on the task of 
flying the drone as planned.
To do this, the control stack
generates the inputs for the motors attached to the drone's rotors,
which consequently track the latest reference state
(equ. \ref{eq:ref_state})
from the planning module.
In the simulations of this thesis,
the control stack is realized with the RPG Quadrotor Control \footnote{
        \url{https://github.com/uzh-rpg/rpg_quadrotor_control}, visited on 17/08/2022
} implementation
(see figure \ref{fig:control_module}).
Since this thesis centers on the reasoning aspect of autonomous navigation, 
only an overview of the deployed control stack is presented here.
The reader may consult the provided references for more details
on the control.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{own/control_module.drawio.pdf}
    \caption[
        The control stack in simulation.
    ]{
        The control stack in simulation.
        \label{fig:control_module}}
\end{figure}

The RPG Quadrotor Control implementation,
which includes the autopilot, the SBUS bridge and and the RPG RotorS interface,
basically executes two feedback control loops in a cascade.
The autopilot integrates the position controller 
that runs the control algorithm of Faessler at al. proposed in \cite{Faessler2018}.
Based on the latest reference state and the fed back drone state estimates,
the position controller generates high-level control commands.
A control command comprises 
the collective thrust of the drone's rotors 
as well as the drone's angular velocity and acceleration.
The SBUS bridge converts each incoming control command
into an SBUS message and forwards this message to the
RPG RotorS interface.
The RPG RotorS interface integrates the body-rate controller
that runs the control algorithm of Faessler at al. proposed in \cite{Faessler2017}.
Based on the latest high-level control command 
as well as the fed back drone state and motor speed estimates,
the body-rate controller generates low-level motor speed commands.
These commands are forwarded to RotorS for execution.
RotorS, developed by Furrer et al. \cite{Furrer2016},
is plugged into the Gazebo \footnote{
    \url{https://gazebosim.org/home}, visited on 18/08/2022
}
simulator to model the drone's physics 
and to provide the controllers with
drone state and motor speed estimates.

In real-world, the drone's flight controller would replace the 
RPG RotorS interface in order to generate hardware-specific low-level
motor commands based on the latest SBUS message from the SBUS bridge.








\section{Expert system} \label{sec:expert_system}
%In contrast to the ANN module,
%the expert system derives its navigation decisions 
%not from onboard sensor data but from its knowledge base.
%The task of the ANN module 
%is to infer navigation decisions,
%i.e., the inputs of the planning module,
%from onboard sensor data.
%In order to make meaningful decisions
%that successfully guide the drone through the racetrack,
%the ANN module must be previously trained on training data 
%of sufficient quantity and quality.
%To guarantee both,
%the training data is automatically generated 
%while the drone is flying through the racetrack.
%At training data generation (see XXX),
%the expert system undertakes the task
%of the completely untrained or yet insufficiently trained ANN module
%to make navigation decisions.

In the context of machine learning, 
an expert system is a program that imitates a human expert
in order to solve a problem. 
It comprises a knowledge base,
which stores known facts and rules, and an
inference engine, which infers new facts 
by applying the rules to the known facts \cite{osti_5675197}.

\paragraph*{Problem} $\ $\\
This thesis implements the expert system 
by Kaufmann et al. \cite{Kaufmann2018}
in order to solve the problem of automated navigation decision making
during the generation of training data for the ANN module.
The training dataset is extended with new samples 
while the drone runs the autonomous navigation method to fly through a racetrack.
The expert system checks the latest navigation decision 
made by the yet partially trained ANN module.
If it does not meet certain requirements,
the expert system intervenes with its own navigation decision.
This, first, keeps the drone on course and, second,
triggers the generation of a new training sample labeled 
with the expert system's navigation decision.

\paragraph*{Knowledge base} $\ $\\
While the ANN module infers navigation decisions from onboard sensor data,
the expert system makes navigation decisions based on its knowledge
which includes the following known facts (\textbf{F*}) and rules (\textbf{R*}).
\begin{itemize}
    
    \item [\textbf{F1}] 
    The planning module's waypoint
    \begin{equation}
        \pos[]{\text{ann}}{\wayp}{\grs}{}
    \end{equation}
     with respect to the global reference system
    (see equ. \ref{eq:pl_global_wayp}) 
    that was computed based
    on the ANN module's latest navigation decision
    (see equ. \ref{eq:navigation_decision}).
    

    \item [\textbf{F2}] The drone's latest position and quaternion orientation estimate,
    which are provided by the drone's state estimation system and 
    may correspond to ground truth in the simulation
    \begin{equation}
        \pos[]{\drone}{}{\grs}{}
        ,\quad 
        \quat[]{\drone}{}{\grs}{}.
    \end{equation}
    
    \item [\textbf{F3}] The center points of the gates of the racetrack
    \begin{equation}
        \left( 
            \pos[]{\gate}{\idx[]{}{}{}{}}{\grs}{}
        \right)
        _{\idx[]{}{}{}{} \in \left\{0, ..., \num[]{\gate}{}{}{} - 1 \right\}}
    \end{equation}
    and the initial index to the currently targeted gate to be passed next
    \begin{equation}
        \idx[]{\gate}{\target}{}{} \in \left\{0, ..., \num[]{\gate}{}{}{} - 1 \right\}
        .
    \end{equation}

    \item [\textbf{R1}] Compute the global trajectory of the current racetrack
    \begin{align} \label{eq:glo_traj}
        \pos[]{\glotraj}{}{\grs}{}
        :\ 
        &\left[
            \timepnt[]{\gate}{0}{}{}, 
            \timepnt[]{\gate}{\num[]{\gate}{}{}{}}{}{}
        \right] \rightarrow \mathbb{R}^3
        ;\quad
        \timepnt[]{}{}{}{}
        \mapsto
        \pos[]{\glotraj}{}{\grs}{}(\timepnt[]{}{}{}{})
        .
    \end{align}
    The algorithm of Mellinger and Kumar \cite{Mellinger2011}
    finds the minimum snap (fourth time derivative of position) spline trajectory
    \begin{align}
        \pos[]{\glotraj}{}{\grs}{}(\timepnt[]{}{}{}{})
        = \sum_{i = 0}^{\num[]{\gate}{}{}{} - 1}
        \begin{cases}
            \pos[]{\glotraj}{i}{\grs}{}(\timepnt[]{}{}{}{})
            , 
            &t \in \left[\timepnt[]{\gate}{i}{}{}, \timepnt[]{\gate}{i+1}{}{}\right] \\
            0, & \text{else}
        \end{cases}
    \end{align}
    that, traverses through all gate center points (\textbf{F3}),
    each at its corresponding gate time $\timepnt[]{\gate}{i}{}{}$,
    and reconnects to itself at $t=\timepnt[]{\gate}{\num[]{\gate}{}{}{}}{}{}$ at gate $i=0$.
    The entries of the pieces 
    $\pos[]{\glotraj}{i}{\grs}{}(\timepnt[]{}{}{}{})$
    of the spline are polynomials. 
    The user specifies the polynomial order 
    $\num[\user]{\glotraj}{\text{poly}}{}{}$
    of the pieces and
    the continuity order
    $\num[\user]{\glotraj}{\text{cont}}{}{}$
    of the spline. 
    However, since the goal is to minimize snap,
    it is required that 
    $
    \num[\user]{\glotraj}{\text{poly}}{}{}
    \ge \num[\user]{\glotraj}{\text{cont}}{}{} \ge 4
    $.
    The algorithm performs the following two-step iterative optimization.

    First,
    the optimal polynomial coefficients of the spline pieces
    are found for fixed gate arrival times 
    $\timepnt[]{\gate}{i}{}{}$
    by solving the optimization problem
    \begin{align}
        &\qquad \argmin{\pos[]{\glotraj}{}{\grs}{}}
        \int_{\timepnt[]{\gate}{0}{}{}}^{\timepnt[]{\gate}{\num[]{\gate}{}{}{}}{}{}}
            \left\|
                \pos[\ddddot]{\glotraj}{}{\grs}{}(\timepnt[]{}{}{}{})
            \right\|^2_2
        \text d \timepnt[]{}{}{}{}
        \nonumber 
        \\
        \text{s.t.}\quad
        & \pos[]{\glotraj}{}{\grs}{}\left(\timepnt[]{\gate}{i}{}{}\right) = 
            \pos[]{\gate}{\idx[]{}{}{}{}}{\grs}{},
        &&
            \frac{\text d^j \pos[]{\glotraj}{}{\grs}{}}{\text d \timepnt[]{j}{}{}{}} 
            (\timepnt[]{\gate}{i}{}{}) \text{ defined},
        \nonumber \\
        & \idx[]{}{}{}{} \in \left\{0, ..., \num[]{\gate}{}{}{}\right\},
        && j \in \left\{1, ..., \num[\user]{\glotraj}{\text{cont}}{}{}\right\}.
    \end{align}
    Note that, as the spline is closed, 
    the first and last gate equate 
    $\pos[]{\gate}{\num[]{\gate}{}{}{}}{\grs}{} = \pos[]{\gate}{0}{\grs}{}$.
    Morover, the gate times of the very first iteration are 
    approximated with the distances between the gate center points
    devided by the user-specified maximum speed $\speed[\user]{\glotraj}{\mxm}{}{}$ of the trajectory.
    The above optimization problem
    is temporally and spatially dedimensionalized 
    to increase numeric stability and 
    reformulated as quadratic program,
    which is solved with the Gurobi\footnote{
        \url{https://www.gurobi.com/}, visited on 20/08/2022
    } optimizer.

    Second, the polynomial coefficients of the spline pieces 
    are fixed
    and the inner gate times
    $\timepnt[]{\gate}{i}{}{}$
    are optimized relatively to each other.
    The corresponding optimization problem
    \begin{align}
        &\qquad \argmin{\timepnt[]{\gate}{i}{}{}}
        \int_{\timepnt[]{\gate}{0}{}{}}^{\timepnt[]{\gate}{\num[]{\gate}{}{}{}}{}{}}
            \left\|
                \pos[\ddddot]{\glotraj}{}{\grs}{}(\timepnt[]{}{}{}{})
            \right\|^2_2
        \text d \timepnt[]{}{}{}{}
        \nonumber \\
        \text{s.t.}\quad
        & \timepnt[]{\gate}{i}{}{} < \timepnt[]{\gate}{i+1}{}{},
        \qquad
        \timepnt[]{\gate}{0}{}{},\ \timepnt[]{\gate}{\num[]{\gate}{}{}{}}{}{} \text{ fixed},
        \qquad
        \idx[]{}{}{}{} \in \left\{0, ..., \num[]{\gate}{}{}{} - 1 \right\}
    \end{align}
    is solved by gradient descent with backtracking line search.

    
    
    The two optimization steps are executed iteratively until the cost
    of the first optimization problem converges. 
    Then, the trajectory is temporally and spatially redimensionalized
    and temporally scaled to adhere to 
    the user-specified maximum values in terms of 
    speed
    $\speed[\user]{\glotraj}{\mxm}{}{}$, 
    thrust 
    $\scacc[\user]{\glotraj}{\mxm}{}{}$
    and roll-pitch rate 
    $\scangvel[\user]{\glotraj}{\mxm}{}{}$
    along the trajectory. 
    For later use,
    the expert system samples the positions and speeds
    of the global trajectory
    \begin{equation}
        \left( 
            \pos[]{\glotraj}{\idx[]{}{}{}{}}{\grs}{}
        \right)
        _{\idx[]{}{}{}{} \in \left\{0, ..., \num[]{\glotraj}{}{}{} - 1 \right\}}
        ,\quad
        \left( 
            \speed[]{\glotraj}{\idx[]{}{}{}{}}{\grs}{}
        \right)
        _{\idx[]{}{}{}{} \in \left\{0, ..., \num[]{\glotraj}{}{}{} - 1 \right\}}
    \end{equation}
    with $\speed[]{\glotraj}{\idx[]{}{}{}{}}{\grs}{} = 
    \left\| 
        \pos[\dot]{\glotraj}{\idx[]{}{}{}{}}{\grs}{}
    \right\|_2 $.
    The sampling occurs at the
    user-specified frequency
    $\freq[\user]{\glotraj}{}{}{}$,
    which results in
    $\num[]{\glotraj}{}{}{} = \freq[\user]{\glotraj}{}{}{} \cdot 
    \left(\timepnt[]{\gate}{\num[]{\gate}{}{}{}}{}{}
    - \timepnt[]{\gate}{0}{}{}\right)$
    samples.

    
    \item [\textbf{R2}] If the drone is closer to the currently targeted gate 
    than a user-specified distance
    \begin{equation}
        \left\| 
            \pos[]{\gate}{\idx[]{\gate}{\target}{}{}}{\grs}{} - \pos[]{\drone}{}{\grs}{}
        \right\|_2 
        < 
        \dist[\user]{\dronetogate}{}{}{},
    \end{equation}
    increment the index to the currently targeted gate
    \begin{align}
        \idx[]{\gate}{}{}{} &\leftarrow (\idx[]{\gate}{}{}{} + 1) \bmod \num[]{\gate}{}{}{}.
    \end{align}
    
    \item [\textbf{R3}] If the
    planning module's waypoint (\textbf{F1}) computed based on the
    ANN module's latest navigation decision,
    is more distant from the global trajectory (\textbf{R1}) than
    a user-specified margin scaled by the 
    the user-specified drone's maximum speed, i.e.,
    \begin{equation}
        \argmin{i \in \{0, ..., \num[]{\glotraj}{}{}{} - 1\}}
            \left\| 
                \pos[]{\text{ann}}{\wayp}{\grs}{}
                - 
                \pos[]{\glotraj}{i}{\grs}{}
            \right\|_2
            >
            \dist[\user]{\wayptoglotraj}{\mxm}{}{} \cdot \frac{\speed[\user]{\drone}{\mxm}{}{} +1}{5},
    \end{equation}
    the expert system is required to intervene with its own navigation decision.

    
    \item [\textbf{R4}] Update the index 
    $\idx[]{\glotraj}{\proj}{}{} \in \{0, ..., \num[]{\glotraj}{}{}{} - 1\}$ 
    to the projection state,
    i.e., the state of the global trajectory
    onto which the drone's latest position estimate is projected,
    with the following iterative method.
    Figure \ref{fig:expert_system_projection} 
    schematically illustrates the method with a 2D example.
    \begin{enumerate}
        \item Compute the index to the previous state of the global trajectory,
        by decrementing the index to the projection state
        \begin{equation}
            \idx[]{\glotraj}{\prev}{}{} 
            = 
            (\idx[]{\glotraj}{\proj}{}{} - 1 + \num[]{\glotraj}{}{}{}) 
            \bmod 
            \num[]{\glotraj}{}{}{}.
        \end{equation}
        \item Starting from the previous state, 
        compute the vector to the projection state
        \begin{equation} \label{eq:vec_prev_state_2_proj_state}
            \anything[]{}{}{\grs}{}{\underline a}
            = 
            \pos[]{\glotraj}{\idx[]{\glotraj}{\proj}{}{}}{\grs}{}
            - 
            \pos[]{\glotraj}{\idx[]{\glotraj}{\prev}{}{}}{\grs}{}
        \end{equation}
        and the vector to the current drone position
        \begin{equation} \label{eq:vec_prev_state_2_drone}
            \anything[]{}{}{\grs}{}{\underline b}
            = 
            \pos[]{\drone}{}{\grs}{}
            - 
            \pos[]{\glotraj}{\idx[]{\glotraj}{\prev}{}{}}{\grs}{}.
        \end{equation}
        \item If the scalar product of the vectors 
        $\anything[]{}{}{\grs}{}{\underline a}$ 
        and 
        $\anything[]{}{}{\grs}{}{\underline b}$, 
        both normalized by the length of $\anything[]{}{}{\grs}{}{\underline a}$,
        is less than 1
        \begin{align} \label{eq:norm_dot_prod_criterion}
            \frac{
                \anything[]{}{}{\grs}{}{\underline a} 
                \cdot 
                \anything[]{}{}{\grs}{}{\underline b}
            }{  
                \anything[]{}{}{\grs}{}{\underline a} 
                \cdot 
                \anything[]{}{}{\grs}{}{\underline a}
            } < 1,
        \end{align}
        go to the next step. 
        Else, increment the index to the projection state
        \begin{equation}
            \idx[]{\glotraj}{\proj}{}{} 
            \leftarrow 
            (\idx[]{\glotraj}{\proj}{}{} + 1) \bmod \num[]{\glotraj}{}{}{}
        \end{equation}
        and go back to step 1.
        \item If the drone is 
        within a user-specified distance to the projection state 
        \begin{equation} \label{eq:dist_drone_proj_criterion}
            \left\| 
                \pos[]{\drone}{}{\grs}{}
                - 
                \pos[]{\glotraj}{\idx[]{\glotraj}{\proj}{}{}}{\grs}{}
            \right\|_2 
            \le 
            \dist[\user]{\dronetoproj}{}{}{},
        \end{equation}
        the index 
        $\idx[]{\glotraj}{\proj}{}{}$ 
        to the projection state is found.
        Else, set the index 
        to the state of the global trajectory
        which has the minimum distance to the current drone position
        \begin{equation} \label{eq:proj_idx_with_min_dist}
            \argmin{\idx[]{\glotraj}{\proj}{}{}}
            \left\| 
                \pos[]{\drone}{}{\grs}{}
                - 
                \pos[]{\glotraj}{\idx[]{\glotraj}{\proj}{}{}}{\grs}{}
            \right\|_2.
        \end{equation}
        Due to this step,
        the expert system does not require to know
        the initial index 
        to the projection state.

    \end{enumerate}
    
    

    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{own/expert_state_projection_3d.pdf}
        \caption[
            Update of the projection state index
        ]{
            Schematic example
            of the update of the projection state index (\textbf{R4}).
            Known are: the positions (blue points) 
            sampled from the global trajectory (blue dotted line),
            the last index to the projection state (green circle)
            and the current position of the drone (orange circle).
            At an iteration,
            the vector from the previous to the projection state 
            (green arrows, equ. \ref{eq:vec_prev_state_2_proj_state})
            and the vector from the previous to the drone position 
            (orange arrows, equ. \ref{eq:vec_prev_state_2_drone})
            are computed.
            Then, the normalized dot product criterion 
            (annotations, equ. \ref{eq:norm_dot_prod_criterion}) 
            is checked. 
            For iteration \#0-2, the criterion is not met.
            Thus, the index to the projection state is incremented
            and another iteration is started.
            At iteration \#3 the criterion is met and the 
            new index to the projection state (green star) is identified
            (assuming the distance criterion 
            (equ. \ref{eq:dist_drone_proj_criterion}) is also met). 
            Note that finding the index to the projection state 
            only with minimum distance 
            (equ. \ref{eq:proj_idx_with_min_dist}) 
            would have failed here,
            since the so indexed state (blue square) 
            belongs to a later or earlier part of the global trajectory 
            which only intersects the current part.
        \label{fig:expert_system_projection}}
    \end{figure}


    
    \item [\textbf{R5}] 
    Update the index 
    $\idx[]{\glotraj}{v}{}{} \in \{0, ..., \num[]{\glotraj}{}{}{} - 1\}$
    to the speed state,
    i.e., the state of the global trajectory
    that is the reference for the normalized speed
    $\speed[\norm]{\expert}{\desired}{}{}$ component
    of the expert system's navigation decision,
    by finding the first state of the global trajectory 
    that follows the projection state
    with a specific distance.
    \begin{enumerate}
        \item Initialize the searched index with the index to the projection state
        \begin{equation}
            \idx[]{\glotraj}{v}{}{} = \idx[]{\glotraj}{\proj}{}{}.
        \end{equation}

        \item Increment the searched index
        \begin{equation}
            \idx[]{\glotraj}{v}{}{} \leftarrow (\idx[]{\glotraj}{v}{}{} + 1) \bmod \num[]{\glotraj}{}{}{}.
        \end{equation}
        \item If the speed state is further 
        from the projection state than a user-specified distance
        \begin{equation}
            \left\| 
                \pos[]{\glotraj}{\idx[]{\glotraj}{v}{}{}}{\grs}{}
                - 
                \pos[]{\glotraj}{\idx[]{\glotraj}{\proj}{}{}}{\grs}{}
            \right\|_2
            > 
            \dist[\user]{\projtospeed}{}{}{}
            .
        \end{equation}
        the searched index is found.
        Else, go back to step 2.
    \end{enumerate}


    \item [\textbf{R6}] 
    Update the index 
    $\idx[]{\glotraj}{\wayp}{}{} \in \{0, ..., \num[]{\glotraj}{}{}{} - 1\}$
    to the waypoint state,
    i.e., the state of the global trajectory
    that is the reference for the image waypoint
    $\pos[]{\expert}{\wayp}{\irs}{}$
    component of the expert system's navigation decision,
    by finding the first state of the global trajectory 
    that follows the projection state
    with a distance to be computed.
    \begin{enumerate}
        \item Set the distance from the projection to the waypoint state
        to the distance from the drone to the closer of
        either the currently or lastly targeted gate.
        However, a user-specified distance constitutes the lower limit
        \begin{align}
            \dist[]{\projtowayp}{}{}{} &= 
            \maxof{
                \dist[\user]{\projtowayp}{\mnm}{}{}
            }{
                \argmin{i} \left\| 
                    \pos[]{\gate}{i}{\grs}{} 
                    - 
                    \pos[]{\drone}{}{\grs}{}
                \right\|_2
            }, 
            \\
            i &\in \left\{ \idx[]{\gate}{\target}{}{}, 
            (\idx[]{\gate}{\target}{}{} - 1 + \num[]{\gate}{}{}{}) 
            \bmod 
            \num[]{\gate}{}{}{}
            \right\}
            .
        \end{align}
        \item Initialize the searched index with the index to the projection state
        \begin{equation}
            \idx[]{\glotraj}{\wayp}{}{} = \idx[]{\glotraj}{\proj}{}{}.
        \end{equation}
        \item Increment the searched index
        \begin{equation}
            \idx[]{\glotraj}{\wayp}{}{} \leftarrow (\idx[]{\glotraj}{\wayp}{}{} + 1) 
            \bmod \num[]{\glotraj}{}{}{}.
        \end{equation}
        \item If the waypoint state is further from the projection state 
        than the distance computed in step 1
        \begin{equation}
            \left\| 
                \pos[]{\glotraj}{\idx[]{\glotraj}{\wayp}{}{}}{\grs}{}
                - 
                \pos[]{\glotraj}{\idx[]{\glotraj}{\proj}{}{}}{\grs}{}
            \right\|_2
            > 
            \dist[]{\projtowayp}{}{}{},
        \end{equation}
        the searched index is found.
        Else, go back to step 3.
    \end{enumerate}


    


    \item [\textbf{R7}] Compute the normalized speed component
    of the expert system's navigation decision
    as the sampled speed of the speed state
    normalized by the maximum speed of the global trajectory
    \begin{equation}
        \speed[\norm]{\expert}{\desired}{}{}
        = 
        \frac{
            \speed[]{\glotraj}{\idx[]{\glotraj}{v}{}{}}{\grs}{}
        }
        {
            \argmax{i \in \{0, ..., \num[]{\glotraj}{}{}{} - 1\}}
            \left\| 
                \speed[]{\glotraj}{\idx[]{}{}{}{}}{\grs}{}
            \right\|_2
        }  
        \in [0,1].
    \end{equation}


    
    
    
    \item [\textbf{R8}] Compute the image waypoint component
    of the expert system's navigation decision
    by applying the transformation
    from the global to the image reference system (see equ. \ref{eq:global_image_transformations})
    on the sampled position of the waypoint state 
    \begin{equation}
        \pos[]{\expert}{\wayp}{\irs}{}
        =
        \trafo[]{}{\irs\grs}{}{} \left(
            \pos[]{\glotraj}{\idx[]{\glotraj}{\wayp}{}{}}{\irs}{}
        \right)
        .
    \end{equation}

\end{itemize}





\paragraph*{Inference Engine} $\ $\\
The inference engine of the expert system is only activated 
during training data generation.
Figure ?? shows the related interaction of the inference engine 
within the autnomous navigation method.
Internally, the inference engine runs the following schedule.

Before the drone starts to fly,
the inference engine pre-computes the global trajectory (\textbf{R1})
and samples the position and speeds.
During the flight, it constantly update the currently targeted
gate index (\textbf{R2}).
Whenever the planning module has computed a global waypoint on the basis
of the latest ANN navigation decision,
the inference engine checks whether it must intervene (\textbf{R3}).
If so, the engine updates its indices to relevant states of the global trajectory
(\textbf{R4-6})
and makes its own navigation decision (\textbf{R7-8}).
Finally the engine sends its navigation decision to the planning module for processing.

\section{Racing vs. Training Data Generation}





